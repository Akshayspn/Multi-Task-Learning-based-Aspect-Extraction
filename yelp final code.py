# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a311VWAwfwKDMhH0FpBLP71O5884IF57
"""

!pip install datasets spacy transformers
!python -m spacy download en_core_web_sm

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    BertTokenizerFast,
    BertForTokenClassification,
    BertForMaskedLM,
    AdamW,
    get_scheduler
)
from datasets import Dataset
import spacy

# Load SpaCy's small language model
nlp = spacy.load("en_core_web_sm")

# Load Tokenizer using BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Models
class MultiTaskModel(nn.Module):
    def __init__(self, pretrained_model_name="bert-base-uncased"):
        super().__init__()
        self.shared_encoder = BertForTokenClassification.from_pretrained(pretrained_model_name)
        self.token_classification_head = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(768, 3)
        )
        self.mlm_head = BertForMaskedLM.from_pretrained(pretrained_model_name).cls

    def forward(self, input_ids, attention_mask, token_classification_labels=None, mlm_labels=None):
        shared_output = self.shared_encoder.bert(input_ids, attention_mask=attention_mask, return_dict=True)
        token_classification_logits = self.token_classification_head(shared_output.last_hidden_state)
        mlm_logits = self.mlm_head(shared_output.last_hidden_state)

        loss = 0
        if token_classification_labels is not None:
            class_weights = torch.tensor([1.0, 2.0, 2.0]).to(device)
            loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)
            token_classification_loss = loss_fn(token_classification_logits.view(-1, 3), token_classification_labels.view(-1))
            loss += token_classification_loss

        if mlm_labels is not None:
            mlm_loss = nn.CrossEntropyLoss(ignore_index=-100)(mlm_logits.view(-1, tokenizer.vocab_size), mlm_labels.view(-1))
            loss += mlm_loss

        return loss, token_classification_logits, mlm_logits


class BERTBIOTagger(nn.Module):
    def __init__(self, pretrained_model_name="bert-base-uncased", num_labels=3):
        super().__init__()
        self.model = BertForTokenClassification.from_pretrained(pretrained_model_name, num_labels=num_labels)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss if labels is not None else None
        logits = outputs.logits
        return loss, logits


class StandaloneMLM(nn.Module):
    def __init__(self, pretrained_model_name="bert-base-uncased"):
        super().__init__()
        self.model = BertForMaskedLM.from_pretrained(pretrained_model_name)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.model(input_ids, attention_mask, labels=labels)
        loss = outputs.loss if labels is not None else None
        logits = outputs.logits
        return loss, logits


class BiLSTMCRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim=300, hidden_dim=512, output_dim=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

    def forward(self, input_ids):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)
        return logits


# Dynamic BIO Tagging Logic
def label_bio_tags_dynamic(text):
    doc = nlp(text)
    tokens = [token.text for token in doc]
    tags = ["O"] * len(tokens)

    for i, token in enumerate(doc):
        if token.pos_ in {"NOUN", "ADJ"} and (token.dep_ in {"amod", "nsubj", "dobj"}):
            tags[i] = "B" if tags[i] == "O" else "I"
        if token.pos_ in {"AUX", "VERB"} and token.dep_ == "ROOT" and "neg" in [child.dep_ for child in token.children]:
            for child in token.children:
                if child.dep_ in {"attr", "acomp", "xcomp", "pobj"}:
                    child_index = tokens.index(child.text)
                    tags[child_index] = "B" if tags[child_index] == "O" else "I"

    label_mapping = {"O": 0, "B": 1, "I": 2}
    return [label_mapping[tag] for tag in tags]


# Data Loading and Preprocessing
def load_and_preprocess_data():
    dataset = load_dataset("yelp_polarity")
    train_df = pd.DataFrame(dataset['train'])
    reviews_df = train_df
    reviews_df["bio_tags"] = reviews_df["text"].apply(label_bio_tags_dynamic)
    dataset = Dataset.from_pandas(reviews_df).shuffle(seed=42)

    total_size = len(dataset)
    train_size = int(0.75 * total_size)
    val_size = int(0.15 * total_size)

    train_data = dataset.select(range(train_size))
    val_data = dataset.select(range(train_size, train_size + val_size))
    test_data = dataset.select(range(train_size + val_size, total_size))

    return train_data, val_data, test_data


def create_dataloader(dataset, tokenizer, batch_size=16):
    def tokenize_function(examples):
        tokenized = tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128, return_tensors="pt")
        labels = []
        for i, bio_tags in enumerate(examples["bio_tags"]):
            word_ids = tokenized.word_ids(batch_index=i)
            aligned_labels = [-100] * len(tokenized['input_ids'][i])
            for j, word_id in enumerate(word_ids):
                if word_id is not None and word_id < len(bio_tags):
                    aligned_labels[j] = bio_tags[word_id]
            labels.append(aligned_labels)
        tokenized["labels"] = labels
        return tokenized

    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    tokenized_datasets.set_format(type="torch", columns=['input_ids', 'attention_mask', 'labels'])
    return DataLoader(tokenized_datasets, batch_size=batch_size)


def train_model(model, train_loader, val_loader, device, tokenizer, max_epochs=1):
    optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)
    scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * max_epochs)
    model.to(device)

    for epoch in range(max_epochs):
        model.train()
        total_loss = 0
        for batch in tqdm(train_loader, desc=f"Training Epoch {epoch+1}"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            if isinstance(model, MultiTaskModel):
                loss, _, _ = model(input_ids, attention_mask, token_classification_labels=labels)
            elif isinstance(model, (BERTBIOTagger, StandaloneMLM)):
                loss, _ = model(input_ids, attention_mask, labels=labels)
            elif isinstance(model, BiLSTMCRF):
                logits = model(input_ids)
                loss = nn.CrossEntropyLoss()(logits.view(-1, 3), labels.view(-1))
            else:
                raise ValueError(f"Unsupported model type: {type(model)}")

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        print(f"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}")


def evaluate_model(model, data_loader, device, tokenizer, task="evaluation"):
    model.eval()
    all_predictions, all_labels = [], []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc=f"Evaluating {task}"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            if isinstance(model, MultiTaskModel):
                _, token_classification_logits, _ = model(input_ids, attention_mask)
            elif isinstance(model, BiLSTMCRF):
                token_classification_logits = model(input_ids)
            else:
                _, token_classification_logits = model(input_ids, attention_mask)

            predictions = torch.argmax(token_classification_logits, dim=-1).cpu().numpy()
            true_labels = labels.cpu().numpy()

            all_predictions.extend(predictions.flatten())
            all_labels.extend(true_labels.flatten())

    f1 = f1_score(all_labels, all_predictions, average='weighted')
    precision = precision_score(all_labels, all_predictions, average='weighted')
    recall = recall_score(all_labels, all_predictions, average='weighted')
    accuracy = accuracy_score(all_labels, all_predictions)

    print(f"{task} - F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}")


# Main Workflow
train_data, val_data, test_data = load_and_preprocess_data()
train_loader = create_dataloader(train_data, tokenizer)
val_loader = create_dataloader(val_data, tokenizer)
test_loader = create_dataloader(test_data, tokenizer)

# Initialize Models
multi_task_model = MultiTaskModel()
bert_bio_tagger = BERTBIOTagger()
standalone_mlm = StandaloneMLM()
bilstm_crf = BiLSTMCRF(vocab_size=tokenizer.vocab_size)

# Train and Evaluate Models
print("\n=== Training MultiTaskModel ===")
train_model(multi_task_model, train_loader, val_loader, device, tokenizer)
evaluate_model(multi_task_model, test_loader, device, tokenizer, task="MultiTaskModel Evaluation")

print("\n=== Training BERTBIOTagger ===")
train_model(bert_bio_tagger, train_loader, val_loader, device, tokenizer)
evaluate_model(bert_bio_tagger, test_loader, device, tokenizer, task="BERTBIOTagger Evaluation")

print("\n=== Training StandaloneMLM ===")
train_model(standalone_mlm, train_loader, val_loader, device, tokenizer)
evaluate_model(standalone_mlm, test_loader, device, tokenizer, task="StandaloneMLM Evaluation")

print("\n=== Training BiLSTMCRF ===")
train_model(bilstm_crf, train_loader, val_loader, device, tokenizer)
evaluate_model(bilstm_crf, test_loader, device, tokenizer, task="BiLSTMCRF Evaluation")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
df=reviews_df

short_sentences = []
for index, row in df.iterrows():
    sentence = row['text']
    if len(sentence.split()) < 30:
        short_sentences.append(sentence)
    if len(short_sentences) == 30:
        break

# Convert to a set to remove duplicates (maintaining order)
unique_short_sentences = list(dict.fromkeys(short_sentences))

# If you still have less than 20 unique sentences after removing duplicates
while len(unique_short_sentences) < 30:
    print("Not enough unique sentences. Add more sentences to the dataframe 'df' or lower the length requirement.")
    break

# Print or use the unique short sentences
for sentence in unique_short_sentences:
  print(sentence)

def visualize_attention_heatmap(model, sentence, tokenizer, device):
    model.eval()
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=164).to(device)
    with torch.no_grad():
        outputs = model.shared_encoder.bert(**inputs, output_attentions=True)

    attention = outputs.attentions[-1].mean(dim=1).squeeze(0).cpu().numpy()

    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

    # Find the indices of [CLS] and [SEP] tokens
    cls_index = tokens.index('[CLS]')
    sep_index = tokens.index('[SEP]')

    # Remove [CLS] and [SEP] tokens and corresponding attention weights
    tokens = [token for i, token in enumerate(tokens) if i != cls_index and i != sep_index]
    attention = np.delete(attention, cls_index, axis=0)
    attention = np.delete(attention, cls_index, axis=1)

    # Update sep_index after removing [CLS]
    sep_index -= 1  # Adjust sep_index as it shifts by one after deleting [CLS]

    attention = np.delete(attention, sep_index, axis=0) # Now use updated sep_index
    attention = np.delete(attention, sep_index, axis=1)


    num_tokens = len(tokens)

    plt.figure(figsize=(10, 8))
    sns.heatmap(attention[:num_tokens, :num_tokens], annot=True, fmt=".2f", xticklabels=tokens[:num_tokens], yticklabels=tokens[:num_tokens], cmap="viridis")
    plt.title("Attention Weights Heatmap")
    plt.xlabel("Tokens")
    plt.ylabel("Tokens")
    plt.xticks(rotation=90)
    plt.show()

for i in unique_short_sentences:
    sentence = i
    print(f"Visualizing attention for sentence: {sentence}")
    visualize_attention_heatmap(multi_task_model, sentence, tokenizer, device)

# prompt: generate readme file for all of above code. This project is aimed sat identifying implicit and explict aspects for aspect based sentiment analysis. Explain in detail in read me file everything.


Additionally, you need to authenticate with your Google Drive and Google Sheets.

## Data Loading and Preprocessing

The code utilizes the Yelp polarity dataset from the `datasets` library.  The data is loaded, preprocessed, and split into training, validation, and testing sets.

A key preprocessing step is the dynamic assignment of BIO (Beginning, Inside, Outside) tags to each token in the text.  This labeling process utilizes spaCy for part-of-speech (POS) tagging and dependency parsing to identify potential aspects based on linguistic features such as nouns, adjectives, verbs, and their grammatical relationships within the sentence.


## Model Architectures

The project explores several model architectures:

1.  **MultiTaskModel:** This model uses a shared BERT encoder for two tasks: token classification (BIO tagging) and masked language modeling (MLM).  This allows the model to learn contextual representations beneficial for both tasks.

2.  **BERTBIOTagger:** A standalone BERT model fine-tuned specifically for token classification.

3.  **StandaloneMLM:** A standalone BERT model fine-tuned for masked language modeling.

4.  **BiLSTMCRF:** A bidirectional LSTM with a Conditional Random Field (CRF) layer.  This model is employed as a traditional sequence labeling approach for comparison.


## Training and Evaluation

Each model is trained using the AdamW optimizer with a linear learning rate scheduler.  The training process is monitored using the loss function.

After training, each model is evaluated on the test set using metrics such as F1-score, precision, recall, and accuracy.

## Visualization of Attention

The project includes a function to visualize the attention weights of the MultiTaskModel.  For a sample of short sentences, this function generates attention heatmaps for every sentence in `unique_short_sentences`, providing insights into the tokens and word relationships that the model focuses on.


## Usage

1.  **Authentication:**  Authenticate with Google Drive and Sheets using the provided authentication code.
2.  **Execution:** Execute the Python script.
3.  **Analysis:** Examine the training loss, evaluation metrics, and attention heatmaps.

## Example

The code includes a loop that displays short sentences from the dataset.

## File Structure

The code provided is in a single python file.  No special folder structure or other files are needed.

## Future Improvements
*   **Hyperparameter Tuning:** Conduct more extensive hyperparameter tuning to potentially improve model performance.
*   **Data Augmentation:** Explore different data augmentation techniques to improve model robustness.
*   **Advanced Attention Visualization:**  Implement more sophisticated methods for analyzing and visualizing attention weights.
*   **Aspect Extraction and Sentiment Analysis:**  Extend the project to perform complete aspect extraction and sentiment analysis tasks.


## Google Colab Integration

The code snippets and instructions provided in this file indicate that this project is intended to be run in a Google Colab environment.